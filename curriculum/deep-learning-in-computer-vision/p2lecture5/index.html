<!-- build time:Mon Aug 21 2023 20:02:24 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="msvalidate.01" content="392F7C2BE2FB337CDDF804BE9EA4A533"><meta name="baidu-site-verification" content="code-hxZ8Vdn0py"><meta name="baidu-site-verification" content="code-d9wI4pQtCL"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="身如蘆葦" href="https://www.johng-mit.cn/rss.xml"><link rel="alternate" type="application/atom+xml" title="身如蘆葦" href="https://www.johng-mit.cn/atom.xml"><link rel="alternate" type="application/json" title="身如蘆葦" href="https://www.johng-mit.cn/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="Machine Learning,Deep Learning,Computer Vision"><link rel="canonical" href="https://www.johng-mit.cn/curriculum/deep-learning-in-computer-vision/p2lecture5/"><title>Part Ⅱ | Lecture 5 | Neural vision advanced applications - Deep Learning in Computer Vision - 课程 | Feast = 身如蘆葦 = 人間似夢</title><meta name="generator" content="Hexo 6.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">Part Ⅱ | Lecture 5 | Neural vision advanced applications</h1><div class="meta"><span class="item" title="Created: 2022-08-12 09:17:47"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">Posted on</span> <time itemprop="dateCreated datePublished" datetime="2022-08-12T09:17:47+08:00">2022-08-12</time> </span><span class="item" title="Symbols count in article"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">Symbols count in article</span> <span>8.7k</span> <span class="text">words</span> </span><span class="item" title="Reading time"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">Reading time</span> <span>22 mins.</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Feast</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="preload" as="style" href="$CSS&display=swap"><link rel="stylesheet" href="$CSS&display=swap" media="print" onload='this.media="all"'></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://johng-mit.cn/wallpaper/marina-reich-Lh6G7kMdGr0-unsplash.jpg"></li><li class="item" data-background-image="https://johng-mit.cn/wallpaper/dan-cristian-padure-dxW4gNp4XhA-unsplash.jpg"></li><li class="item" data-background-image="https://johng-mit.cn/wallpaper/birmingham-museums-trust-NspHfyZnMBE-unsplash.jpg"></li><li class="item" data-background-image="https://johng-mit.cn/wallpaper/birmingham-museums-trust-RRn7VvZCbas-unsplash.jpg"></li><li class="item" data-background-image="https://johng-mit.cn/wallpaper/mark-hang-fung-so-H8zTd5cqxqE-unsplash.jpg"></li><li class="item" data-background-image="https://johng-mit.cn/wallpaper/tamara-menzi-n-vnWQmmVoY-unsplash.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">Home</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/curriculum/" itemprop="item" rel="index" title="In 课程"><span itemprop="name">课程</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/curriculum/Deep-Learning-in-Computer-Vision/" itemprop="item" rel="index" title="In Deep Learning in Computer Vision"><span itemprop="name">Deep Learning in Computer Vision</span></a><meta itemprop="position" content="2"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="en"><link itemprop="mainEntityOfPage" href="https://www.johng-mit.cn/curriculum/deep-learning-in-computer-vision/p2lecture5/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="John G"><meta itemprop="description" content="人間似夢, 風遂人愿，萬事皆好。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="身如蘆葦"></span><div class="body md" itemprop="articleBody"><h1 id="lecture-outline"><a class="anchor" href="#lecture-outline">#</a> Lecture Outline</h1><ul><li>Multi-object detection (YOLO, FasterRCNN)</li><li>Region proposal networks and tracking</li><li>Semantic and instance segmentation</li><li>Self-supervised depth estimation</li><li>Autonomous control and robotics</li></ul><h1 id="object-detection"><a class="anchor" href="#object-detection">#</a> Object Detection</h1><h2 id="what-is-object-detection"><a class="anchor" href="#what-is-object-detection">#</a> What is object detection?</h2><p>Instead of predicting just one label, we want to predict two things: We're going to predict a box, which means the location of that label and the type of that label.</p><p><img data-src="https://johng-mit.cn/img/20220815155939.png" alt=""></p><p>Two things:</p><ul><li>The positions of the box (x, y, h, w)</li><li>The object within the box</li></ul><h2 id="a-simple-solution"><a class="anchor" href="#a-simple-solution">#</a> A simple solution</h2><p>Previously we couldn't change the number outputs in our CNN, because it was always fixed according to the architecture.</p><p><img data-src="https://johng-mit.cn/img/20220815160710.png" alt=""></p><p>We pick a random box at this image, pass this through CNN, and try to classify what is in this box. Basically then we take a different box and we repeat this process. If the CNN <strong>identifies an object</strong>, then we <strong>store the box</strong>. If it doesn't identify the object, then we kind of <strong>ignore</strong> it, and we go on to the next box.</p><p><img data-src="https://johng-mit.cn/img/20220815161336.png" alt=""></p><div class="note warning"><p><strong>Problem:</strong><br>Explosion of number of inputs. Too many sizes, scales, positions. Each time we see a brand new image, we need to repeat this process all over again and start all over.</p></div><h2 id="r-cnn-region-proposal-network-region-evolutional-neural-network"><a class="anchor" href="#r-cnn-region-proposal-network-region-evolutional-neural-network">#</a> R-CNN: Region Proposal Network / Region Evolutional Neural Network</h2><p><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzY5MDk0NzU=">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</span></p><p><img data-src="https://johng-mit.cn/img/20220815162849.png" alt=""></p><p>Instead of just randomly picking boxes, this model will take the input image and will try to extract what are called proposal regions, or region proposals.</p><p>The idea of step number two is to propose all of the potential different boxes(around 2000) that might have something interesting in the image. We shrink them all to the same size. All the regions(boxes) get fed in through the CNN in parallel, and we put classifications on all 2000 boxes.</p><div class="note success"><p><strong>This is a much faster way:</strong></p><ol><li>We're not going through all of the boxes.</li><li>We're able to leverage parallelization of GPU by combining them all into the same shape. We warp them all to the same shape, and therefore we're able to feed them all together simultaneously through the CNN.</li></ol></div><div class="note warning"><p><strong>This is a very bad solution in practice!</strong></p><p><img data-src="https://johng-mit.cn/img/20220815184157.png" alt=""></p><ol><li>Even though the testing process can be done in parallel, the training process actually still has to train on each one of those 2000 boxes in sequence, one after another.</li><li>The region we proposed before are variable size, but for some reason, we had to strength them into the same shape. Thus we lose a lot of the important information about the structure of the image.</li></ol></div><h2 id="faster-r-cnn"><a class="anchor" href="#faster-r-cnn">#</a> Faster R-CNN</h2><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE1MDYuMDE0OTc=">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</span></p><p>Instead of trying to have an external neural network, or saw an external algorithm propose those regions, now we try to learn the regions directly within the same neural network, so we can have this whole process end to end, and we don't have to deal with all of these different 2000 regions in parallel.</p><p>The way this works is that we'll have our big image, and instead of feeding small regions through a CNN, we feed the entire image through a few convolutional layers. The output here will be feature maps on the entire image. <strong>This is important because we're not learning convolutional features over small parts or small boxes here.</strong> We're learning feature maps over the entire image. By looking at the feature maps, the neural network can actually learn to kind of highlight where are the important parts. We can use a region proposal network that will take these feature maps and basically extract regions where these features are being activated. If the activations are inaccurate, the neural network will get that error signal back, propagate and adjust the activations on the next time of training, so that the region proposal network will improve the proposals on the subsequent iterations. Each of these region proposals that are learned by the activations of the neural network can now be fed through their independent convolutional neural networks that actually perform the classification on each one of these.</p><p>A few very important points about this algorithm that made it so strong and so state of art:</p><ol><li>It's extremely fast because we only feed in the entire image directly to this first conventional extractor instead of feeding in the boxes.</li><li>The region proposal network actually is learning to output, predicting the proposals. This means we're not relying on some non-neural-network based algorithms to learn these proposals. We're directly learning how to identify and localize these boxes as part of feature extraction phase. This makes the whole object detection pipeline of this neural network, what we call end to end. We're not learning each part seperately, instead, we learn all of these features together. The neural network has to adjust itself and learn if one part is broken. Actually all of the parts have to kind of contribute to fixing that problem.</li><li>We actually continue extracting features over these proposals. Once we've identified and proposed these regions, the neural network is still building more features and depth along these regions as it goes deeper.</li><li>Each proposal can be fed into its independent classifier to perform the detection.</li></ol><p><img data-src="https://johng-mit.cn/img/20220815200541.png" alt=""></p><p>The second step above is the main contribution of this type of algorithm.</p><h3 id="region-proposal-networkrpn"><a class="anchor" href="#region-proposal-networkrpn">#</a> Region Proposal Network(RPN)</h3><p>After feeding the imput image into conventional layers(or residual layers), we want to take the feature maps and learn proposal regions.</p><p>The proposal region is the anchor of this region.</p><ol><li>It's where the region is (where)</li><li>It's kind of the class of the region (what)</li></ol><p>The center area of each of these anchors is going to be maintained throughout the depth of the network. This is because this network is fully convolutional. We're never shrinking the dimensionality of our features. Our features are actually going to mirror the original input size.</p><p><img data-src="https://johng-mit.cn/img/20220815204838.png" alt=""></p><p>The goal is to have this network identify variable number of anchor boxes. An anchor box is going to be defined by the center of this box(center coordinate), the width and the height of that box.</p><p>First, we want to identify the class of the box depending on <strong>either background object or foreground object</strong>. We're just trying to <strong>classify two classes</strong>, just background or foreground. If an object is classified as fore ground, then this object will be proposed. It will be passed forward into the classification layers to figure out what type of object it is.</p><p>Second, we do the regression of the position, height and the width of the anchor boxes.</p><p>To do these two things, we need to have two lost functions. We combine objective number one with objective number two, in the form of two different loss funcitons.</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mo stretchy="false">{</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">}</mo><mo separator="true">,</mo><mo stretchy="false">{</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">}</mo><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><msub><mi>N</mi><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msub></mfrac><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>L</mi><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo separator="true">,</mo><msubsup><mi>p</mi><mi>i</mi><mo>∗</mo></msubsup><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><mfrac><mn>1</mn><msub><mi>N</mi><mrow><mi>r</mi><mi>e</mi><mi>g</mi></mrow></msub></mfrac><munder><mo>∑</mo><mi>i</mi></munder><msubsup><mi>p</mi><mi>i</mi><mo>∗</mo></msubsup><msub><mi>L</mi><mrow><mi>r</mi><mi>e</mi><mi>g</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msubsup><mi>t</mi><mi>i</mi><mo>∗</mo></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(\{p_i\}, \{t_i\}) = \frac{1}{N_{cls}}\sum_i L_{cls}(p_i, p_i^*) + \lambda \frac{1}{N_{reg}}\sum_i p_i^* L_{reg}(t_i, t_i^*)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">}</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">}</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.599109em;vertical-align:-1.277669em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.3139999999999996em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.10903em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:.01968em">l</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.8360000000000001em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em"><span style="top:-1.872331em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:.01968em">l</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7386959999999999em"><span style="top:-2.4530000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:2.599109em;vertical-align:-1.277669em"></span><span class="mord mathnormal">λ</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.10903em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:.03588em">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.972108em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em"><span style="top:-1.872331em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7386959999999999em"><span style="top:-2.4530000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:.03588em">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7386959999999999em"><span style="top:-2.4530000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{p_i\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> is the loss with respect to the probability of this being an object and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{t_i\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> is the loss with respect to the coordinates of this object(could be thought of as kind of like the target space of this object).</p><p>The first step is to classify objective number one, classify as background or foreground. To do this, we can literally take labels of the scene and check what percentage of our ground truth label is actually identifying an object within this region proposal. We can literally use a binary softmax layer or a binary sigmoid cross entropy layer to classify the probability from our predicted probability and from our dataset.</p><p>The second step is going to focus on the regression and the modification of the size and the position of each box. Imagine the predicted position is a little bit shifted from the ground truth position. We can take a mean squared error loss, subtract the coordinates from our ground truth dataset to the closest predicted box, and regress on the error between these two boxes. The closest box to the one that we've predicted will move towards it on the next iteration of training.</p><h1 id="semantic-segmentation"><a class="anchor" href="#semantic-segmentation">#</a> Semantic segmentation</h1><p>Instead of classifying for a box, we actually classify every single pixel in the original image.</p><p><img data-src="https://johng-mit.cn/img/20220816085338.png" alt=""></p><p>This essentially means we will take the pixel on the top left, and we want to output another image where that same pixel in the right hand image, corresponds to the label, i.e., the object class of that pixel.</p><h2 id="fully-convolutional-neural-networks-fcnn"><a class="anchor" href="#fully-convolutional-neural-networks-fcnn">#</a> Fully convolutional neural networks (FCNN)</h2><p><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzc0NzgwNzI=">Fully Convolutional Networks for Semantic Segmentation</span></p><p>For the first part of this model, convolutional layers followed by max pooling layers followed by nonlinearities and repeat.</p><p>For the second part, we want to upscale, which is called unpooling, and is the same as bilinear sampling.</p><p>Two different ways to do unpooling operation:</p><ol><li>Nearest neighbor unpooling: Assume we want to double the sides of image, we can duplicate one pixel four times in the small quadrant.</li><li>Bed of nails: Drop in certain pieces and certain pixels to the larger image, and keep everything else at zero. The goal of the next conventional layer is to learn how to interpolate and combine all of this information together.</li></ol><p><img data-src="https://johng-mit.cn/img/20220816101540.png" alt=""></p><h3 id="the-importance-of-skip-connections"><a class="anchor" href="#the-importance-of-skip-connections">#</a> The importance of skip connections</h3><p>In these types of networks, the middle part is very small, so a lot of information can be lost.</p><p><strong>Loss function:</strong><br>Binary cross entropy loss minimizes distance between ground truth and predicted probability distributions.</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo>−</mo><munder><mo>∑</mo><mrow><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi></mrow></munder><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">-\sum_{class} y_{true} \log(y_{pred})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.3521180000000004em;vertical-align:-1.3021129999999999em"></span><span class="mord">−</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000005em"><span style="top:-1.847887em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:.01968em">l</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span></span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021129999999999em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p>We do the same object classification for every pixel. But because of the middle layer is very small, we're not able to achieve very accurate predictions on the boundaries.</p><p><strong>Problem:</strong><br>The encoder reduces the dimensionality of our input and makes it difficult for the decoder to capture low-level details.</p><p>To solve this, we make every layer also connected to the same sized alyer in the future part of the network.</p><p><img data-src="https://johng-mit.cn/img/20220816133855.png" alt=""></p><h3 id="remember-residual-layers"><a class="anchor" href="#remember-residual-layers">#</a> Remember: Residual layers</h3><p>Instead of trying to learn the entire function from x to y, we broke it up to learn a lot of changes in x to get to y. It allow us to make training a lot smoother for the neural network, and improve the quality of the solutions that were learned.</p><h2 id="u-net"><a class="anchor" href="#u-net">#</a> U-Net</h2><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE1MDUuMDQ1OTc=">U-Net: Convolutional Networks for Biomedical Image Segmentation</span></p><p><img data-src="https://johng-mit.cn/img/20220816135646.png" alt=""></p><h3 id="depth-estimation-using-u-net"><a class="anchor" href="#depth-estimation-using-u-net">#</a> Depth estimation using U-Net</h3><p><img data-src="https://johng-mit.cn/img/20220816135837.png" alt=""></p><p><img data-src="https://johng-mit.cn/img/20220816140438.png" alt=""></p><p>We as humans have two eyes, we compute basically the distance, or the divergence between two cameras(image that seen in our two eyes). If a pixel in one eye is very close to the pixel in the second eye, this means the point is very far away.</p><p>We can compute the <strong>depth</strong> for every pixel using this type of algorithm.</p><p><img data-src="https://johng-mit.cn/img/20220816140656.png" alt=""></p><p><img data-src="https://johng-mit.cn/img/20220816141048.png" alt=""></p><div class="tags"><a href="/tags/Machine-Learning/" rel="tag"><i class="ic i-tag"></i> Machine Learning</a> <a href="/tags/Deep-Learning/" rel="tag"><i class="ic i-tag"></i> Deep Learning</a> <a href="/tags/Computer-Vision/" rel="tag"><i class="ic i-tag"></i> Computer Vision</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">Edited on</span> <time title="Modified: 2023-01-13 20:31:47" itemprop="dateModified" datetime="2023-01-13T20:31:47+08:00">2023-01-13</time> </span><span id="curriculum/deep-learning-in-computer-vision/p2lecture5/" class="item leancloud_visitors" data-flag-title="Part Ⅱ | Lecture 5 | Neural vision advanced applications" title="Views"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">Views</span> <span class="leancloud-visitors-count"></span> <span class="text">times</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="John G 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="John G 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>Post author: </strong>John G <i class="ic i-at"><em>@</em></i>身如蘆葦</li><li class="link"><strong>Post link: </strong><a href="https://www.johng-mit.cn/curriculum/deep-learning-in-computer-vision/p2lecture5/" title="Part Ⅱ | Lecture 5 | Neural vision advanced applications">https://www.johng-mit.cn/curriculum/deep-learning-in-computer-vision/p2lecture5/</a></li><li class="license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> unless stating additionally.</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/book-note/mml/MML-01/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;johng-mit.cn&#x2F;wallpaper&#x2F;denise-chan-pXmbsF70ulM-unsplash.jpg" title="Mathematics for Machine Learning | Chapter 1"><span class="type">Previous Post</span> <span class="category"><i class="ic i-flag"></i> Mathematics for Machine Learning</span><h3>Mathematics for Machine Learning | Chapter 1</h3></a></div><div class="item right"><a href="/technology/rdp/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;johng-mit.cn&#x2F;wallpaper&#x2F;annie-spratt-kZy1nlnf9Z8-unsplash.jpg" title="使用RD Client + frp内网穿透实现iPad远程控制"><span class="type">Next Post</span> <span class="category"><i class="ic i-flag"></i> 杂项</span><h3>使用RD Client + frp内网穿透实现iPad远程控制</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="Contents"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#lecture-outline"><span class="toc-number">1.</span> <span class="toc-text">Lecture Outline</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#object-detection"><span class="toc-number">2.</span> <span class="toc-text">Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#what-is-object-detection"><span class="toc-number">2.1.</span> <span class="toc-text">What is object detection?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#a-simple-solution"><span class="toc-number">2.2.</span> <span class="toc-text">A simple solution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#r-cnn-region-proposal-network-region-evolutional-neural-network"><span class="toc-number">2.3.</span> <span class="toc-text">R-CNN: Region Proposal Network &#x2F; Region Evolutional Neural Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#faster-r-cnn"><span class="toc-number">2.4.</span> <span class="toc-text">Faster R-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#region-proposal-networkrpn"><span class="toc-number">2.4.1.</span> <span class="toc-text">Region Proposal Network(RPN)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#semantic-segmentation"><span class="toc-number">3.</span> <span class="toc-text">Semantic segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#fully-convolutional-neural-networks-fcnn"><span class="toc-number">3.1.</span> <span class="toc-text">Fully convolutional neural networks (FCNN)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-importance-of-skip-connections"><span class="toc-number">3.1.1.</span> <span class="toc-text">The importance of skip connections</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#remember-residual-layers"><span class="toc-number">3.1.2.</span> <span class="toc-text">Remember: Residual layers</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u-net"><span class="toc-number">3.2.</span> <span class="toc-text">U-Net</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#depth-estimation-using-u-net"><span class="toc-number">3.2.1.</span> <span class="toc-text">Depth estimation using U-Net</span></a></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="Related"><ul><li class="active"><a href="/curriculum/deep-learning-in-computer-vision/p2lecture5/" rel="bookmark" title="Part Ⅱ | Lecture 5 | Neural vision advanced applications">Part Ⅱ | Lecture 5 | Neural vision advanced applications</a></li></ul></div><div class="overview panel" data-title="Overview"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="John G" data-src="/images/avatar.jpg"><p class="name" itemprop="name">John G</p><div class="description" itemprop="description">風遂人愿，萬事皆好。</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">9</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">11</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">8</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL2pvaG5nLW1pdA==" title="https:&#x2F;&#x2F;github.com&#x2F;johng-mit"><i class="ic i-github"></i></span> <span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS9nb3UtZnUtZ3VpLTQ1LTg1" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;gou-fu-gui-45-85"><i class="ic i-zhihu"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTM5MzY5Nzk1NjA=" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;3936979560"><i class="ic i-cloud-music"></i></span> <span class="exturl item email" data-url="bWFpbHRvOmpvaG5nLW1pdEBvdXRsb29rLmNvbQ==" title="mailto:johng-mit@outlook.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li><li class="item"><a href="/about/about-zh/" rel="section"><i class="ic i-user"></i>关于</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/book-note/mml/MML-01/" rel="prev" title="Previous Post"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/technology/rdp/" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>Random Posts</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/%E6%95%B0%E5%AD%A6/" title="In 数学">数学</a> <i class="ic i-angle-right"></i> <a href="/categories/%E6%95%B0%E5%AD%A6/%E6%A6%82%E7%8E%87%E4%B8%8E%E7%BB%9F%E8%AE%A1/" title="In 概率与统计">概率与统计</a></div><span><a href="/math/probability-and-statistics/parameter-estimation/" title="参数估计与假设检验">参数估计与假设检验</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/technology/" title="In 技术">技术</a> <i class="ic i-angle-right"></i> <a href="/categories/technology/%E6%9D%82%E9%A1%B9/" title="In 杂项">杂项</a></div><span><a href="/technology/rdp/" title="使用RD Client + frp内网穿透实现iPad远程控制">使用RD Client + frp内网穿透实现iPad远程控制</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/book-note/" title="In 读书笔记">读书笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/book-note/Mathematics-for-Machine-Learning/" title="In Mathematics for Machine Learning">Mathematics for Machine Learning</a></div><span><a href="/book-note/mml/MML-01/" title="Mathematics for Machine Learning | Chapter 1">Mathematics for Machine Learning | Chapter 1</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/curriculum/" title="In 课程">课程</a> <i class="ic i-angle-right"></i> <a href="/categories/curriculum/Deep-Learning-in-Computer-Vision/" title="In Deep Learning in Computer Vision">Deep Learning in Computer Vision</a></div><span><a href="/curriculum/deep-learning-in-computer-vision/p2lecture5/" title="Part Ⅱ | Lecture 5 | Neural vision advanced applications">Part Ⅱ | Lecture 5 | Neural vision advanced applications</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/curriculum/" title="In 课程">课程</a> <i class="ic i-angle-right"></i> <a href="/categories/curriculum/PKU-HPCGame/" title="In PKU HPCGame">PKU HPCGame</a></div><span><a href="/curriculum/HPC/PKU-HPCGAME-3/" title="PKU HPCGame 讲座笔记（3）- MPI简介（司嘉祺）">PKU HPCGame 讲座笔记（3）- MPI简介（司嘉祺）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/confusion/" title="In 迷惘集">迷惘集</a> <i class="ic i-angle-right"></i> <a href="/categories/confusion/%E8%BF%91%E5%86%B5/" title="In 近况">近况</a></div><span><a href="/confusion/%E8%BF%91%E5%86%B5-2022%E5%B9%B4%E6%98%A5/" title="近况 2022年春">近况 2022年春</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/book-note/" title="In 读书笔记">读书笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/book-note/Mathematics-for-Machine-Learning/" title="In Mathematics for Machine Learning">Mathematics for Machine Learning</a></div><span><a href="/book-note/mml/MML-02/" title="Mathematics for Machine Learning | Chapter 2">Mathematics for Machine Learning | Chapter 2</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/curriculum/" title="In 课程">课程</a> <i class="ic i-angle-right"></i> <a href="/categories/curriculum/PKU-HPCGame/" title="In PKU HPCGame">PKU HPCGame</a></div><span><a href="/curriculum/HPC/PKU-HPCGAME-2/" title="PKU HPCGame 讲座笔记（2）- GPU编程（郝哲文）">PKU HPCGame 讲座笔记（2）- GPU编程（郝哲文）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/curriculum/" title="In 课程">课程</a> <i class="ic i-angle-right"></i> <a href="/categories/curriculum/PKU-HPCGame/" title="In PKU HPCGame">PKU HPCGame</a></div><span><a href="/curriculum/HPC/PKU-HPCGAME-1/" title="PKU HPCGame 讲座笔记（1）- 概论（郭俊逸）">PKU HPCGame 讲座笔记（1）- 概论（郭俊逸）</a></span></li></ul></div><div><h2>Recent Comments</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2022 – <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">John G @ Feast</span></div><div class="record"><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/">苏ICP备2022017653号-1</a></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">55k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">2:17</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"curriculum/deep-learning-in-computer-vision/p2lecture5/",favicon:{show:"（●´3｀●）Goooood",hide:"(´Д｀)Booooom"},search:{placeholder:"Search for Posts",empty:"We didn't find any results for the search: ${query}",stats:"${hits} results found in ${time} ms"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'Copied to clipboard successfully! <br> All articles in this blog are licensed under <i class="ic i-creative-commons"></i>BY-NC-SA.',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->